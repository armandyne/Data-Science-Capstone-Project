---
title: "Capstone Project. Task 1 - Getting and Cleaning Data"
author: "Arman Iskaliyev"
date: "April 10, 2018"
output: 
  html_document: 
    keep_md: yes
---

```{r}
library(stringr, warn.conflicts = FALSE)
library(readr, warn.conflicts = FALSE)
library(purrr, warn.conflicts = FALSE)
library(dplyr, warn.conflicts = FALSE)
```

1. Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.

2. Profanity filtering - removing profanity and other words you do not want to predict.

---------------------------------------------------

##Load data
```{r}
if(!file.exists("./data/raw_data.rda")) {
     blogs_vec <- read_lines("./data/final/en_US/en_US.blogs.txt")
     news_vec <- read_lines("./data/final/en_US/en_US.news.txt")
     twitter_vec <- read_lines("./data/final/en_US/en_US.twitter.txt")
     
     save(blogs_vec, news_vec, twitter_vec, file = "./data/raw_data.rda")
} else {
     load("./data/raw_data.rda")
}

head(blogs_vec, 2)
head(twitter_vec, 2)
head(news_vec, 2)
```

##Data summaries

Size of files:
```{r}
map_dbl(list.files("./data/final/en_US/", 
                   pattern = "\\.txt$", 
                   full.names = TRUE),
        ~file.size(.)/1024/1024) %>% 
     set_names(list.files("./data/final/en_US/", pattern = "\\.txt$")) %>% 
     map_chr(~str_c(as.character(round(.,2)),"Mb"))
```

Number of rows for each file:
```{r}
map_int(list(blogs_vec, twitter_vec, news_vec), length) %>%
     set_names(list.files("./data/final/en_US/", pattern = "\\.txt$"))
```

##Data sampling

```{r}
if(!file.exists("./data/sampled_data.rda")) {
     blogs_sample <- sample(blogs_vec, 10000)
     news_sample <- sample(news_vec, 10000)
     twitter_sample <- sample(twitter_vec, 20000)

     save(blogs_sample, news_sample, twitter_sample, file = "./data/sampled_data.rda")
} else {
     load("./data/sampled_data.rda")
}
```

Tokenize:
```{r}
blogs_sample_tokens <- stringr::words(blogs_sample)

length(blogs_sample_tokens)
length(unique(blogs_sample_tokens))
```